{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What type of stone is formed due to high volume of uric acid in urine?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Uric acid stones are formed due to high levels of uric acid in the urine.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How can we prevent forming them?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To prevent forming uric acid stones, you can take the following steps:\n",
      "\n",
      "1. **Medications**: Your healthcare provider may prescribe medications such as:\n",
      "   - **Allopurinol**: This medication lowers the level of uric acid in the blood and urine, helping to prevent uric acid stones.\n",
      "   - **Potassium citrate**: This medication makes the urine less acidic or more alkaline, which helps prevent uric acid stones.\n",
      "\n",
      "2. **Dietary Changes**:\n",
      "   - **Limit Animal Protein**: Reduce your intake of meat, fish, seafood, poultry, pork, lamb, mutton, and game meat. These foods can increase uric acid levels.\n",
      "   - **Increase Fruits and Vegetables**: A diet rich in fruits and vegetables can help make the urine less acidic.\n",
      "   - **Stay Hydrated**: Drink plenty of fluids, especially water, to increase urine volume and dilute the uric acid.\n",
      "\n",
      "3. **Manage Underlying Conditions**: If you have conditions like gout, type 2 diabetes, or chronic diarrhea, work with your healthcare provider to manage these conditions effectively, as they can contribute to uric acid stone formation.\n",
      "\n",
      "4. **Maintain a Healthy Weight**: Being overweight can increase the risk of uric acid stones, so maintaining a healthy weight through diet and exercise can be beneficial.\n",
      "\n",
      "If you have specific concerns or questions about preventing uric acid stones, it's best to consult with a healthcare provider for personalized advice.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "List down all types of kidney stones mentioned in the document.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The document mentions the following types of kidney stones:\n",
      "\n",
      "1. Calcium stones (including calcium oxalate and calcium phosphate)\n",
      "2. Uric acid stones\n",
      "3. Struvite/infection stones\n",
      "4. Cystine stones\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def get_datetime_str():\n",
    "    # Get the current datetime\n",
    "    current_time = datetime.now()\n",
    "    # Format the datetime string\n",
    "    datetime_str = current_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    return datetime_str\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Step 1: Load PDF documents\n",
    "loader = PyPDFLoader(\"../sample_documents/Kidney-Stones-Patient-Guide.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "full_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "# Step 2: Split text into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=400)\n",
    "docs = text_splitter.create_documents([full_text])\n",
    "\n",
    "# Step 3: Embed and store in vector DB (FAISS)\n",
    "embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\")\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Step 4: Create conversational memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"answer\")\n",
    "\n",
    "# Step 5: Load LLM (ChatGPT 3.5 Turbo or other)\n",
    "llm = ChatGroq(model=\"mistral-saba-24b\", temperature=0)\n",
    "\n",
    "\n",
    "from langchain_core.callbacks import BaseCallbackHandler\n",
    "\n",
    "\n",
    "# Step 6: Create Conversational Retrieval Chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# Step 7: Simulate a chat loop\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"RAG Chatbot (type 'exit' to quit)\")\n",
    "    chat_history = []\n",
    "\n",
    "    timestamp = get_datetime_str()\n",
    "\n",
    "    while True:\n",
    "        query = input(\"\\nUser: \")\n",
    "\n",
    "        if query.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "            \n",
    "        prompts_dir = f\"prompts/{timestamp}/{query}\"\n",
    "        os.makedirs(prompts_dir, exist_ok=True)\n",
    "\n",
    "        llm_call_n = 0\n",
    "        class PromptCallback(BaseCallbackHandler):\n",
    "            def on_llm_start(self, serialized, prompts, **kwargs):\n",
    "                global llm_call_n\n",
    "                open(f\"{prompts_dir}/prompt_{llm_call_n}.txt\", \"a+\").write(prompts[0])\n",
    "                llm_call_n += 1\n",
    "\n",
    "        # Initialize with your chain\n",
    "        callback = PromptCallback()\n",
    "\n",
    "        result = qa_chain.invoke(query, config={\"callbacks\": [callback]})\n",
    "\n",
    "        for msg in result[\"chat_history\"]:\n",
    "            msg.pretty_print()\n",
    "\n",
    "        time.sleep(1)  # Give the output time to show up\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Append to history\n",
    "        chat_history.append((query, result[\"answer\"]))\n",
    "\n",
    "        open(f\"{prompts_dir}/answer.txt\", \"a+\").write(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-generating answers by passing internally generated prompts directly to the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def extract_messages_from_rephrase_prompt(rephrase_prompt_path):\n",
    "    lst = open(rephrase_prompt_path).read().split(\"Human: \")[1:]\n",
    "    messages = []\n",
    "    for i, item in enumerate(lst):\n",
    "        if i == 0:\n",
    "            messages.append({\"role\": \"user\", \"content\": item})\n",
    "        elif i == len(lst)-1:\n",
    "            user_assistant = item.split(\"Assistant: \")\n",
    "            assistant_followup = user_assistant[1].split(\"\\n\")\n",
    "            assistant = \"\\n\".join(assistant_followup[:-2])\n",
    "            followup = \"\\n\".join(assistant_followup[-2:])\n",
    "            messages.append({\"role\": \"user\", \"content\": user_assistant[0]})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "            messages.append({\"role\": \"user\", \"content\": followup})\n",
    "        else:\n",
    "            user_assistant = item.split(\"Assistant: \")\n",
    "            messages.append({\"role\": \"user\", \"content\": user_assistant[0]})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": user_assistant[1]})\n",
    "    return messages\n",
    "\n",
    "def extract_messages_from_final_prompt(final_prompt_path):\n",
    "    text = open(final_prompt_path).read()\n",
    "    system_match = re.search(r\"System:(.*?)Human:\", text, re.DOTALL)\n",
    "    human_match = re.search(r\"Human:(.*?)$\", text, re.DOTALL)\n",
    "    result = {\n",
    "        \"system\": system_match.group(1).strip() if system_match else None,\n",
    "        \"human\": human_match.group(1).strip() if human_match else None,\n",
    "    }\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": result[\"system\"]},\n",
    "        {\"role\": \"user\", \"content\": result[\"human\"]},\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def test_prompts(prompts_dir):\n",
    "    len_prompts = len(os.listdir(prompts_dir))\n",
    "    if len_prompts == 2:\n",
    "        messages = extract_messages_from_final_prompt(f\"{prompts_dir}/prompt_0.txt\")\n",
    "        implicit_ans = open(f\"{prompts_dir}/answer.txt\").read()  # ANSWER BY PASSING PROMPT IMPLICITLY\n",
    "        explicit_ans = llm.invoke(input=messages).content  # ANSWER BY PASSING PROMPT EXPLICITLY\n",
    "\n",
    "        if implicit_ans == explicit_ans:\n",
    "            print(\"Both answers, from langchain and by passing final prompt to llm are the same!\")\n",
    "    else:\n",
    "        messages = extract_messages_from_rephrase_prompt(f\"{prompts_dir}/prompt_0.txt\")\n",
    "        response = llm.invoke(input=messages)\n",
    "        implicit_rephrased_question = open(f\"{prompts_dir}/prompt_1.txt\").read().split(\"Human: \")[-1]  # REPHRASED QUESTION BY PASSING PROMPT IMPLICITLY\n",
    "        explicit_rephrased_question = response.content  # REPHRASED QUESTION BY PASSING PROMPT EXPLICITLY\n",
    "\n",
    "        if implicit_rephrased_question == explicit_rephrased_question:\n",
    "            print(\"Both rephrased questions, from langchain and by passing rephrase prompt to llm are the same!\")\n",
    "        \n",
    "        messages = extract_messages_from_final_prompt(f\"{prompts_dir}/prompt_1.txt\")\n",
    "        implicit_ans = open(f\"{prompts_dir}/answer.txt\").read()\n",
    "        explicit_ans = llm.invoke(input=messages).content\n",
    "\n",
    "        if implicit_ans == explicit_ans:\n",
    "            print(\"Both answers, from langchain and by passing final prompt to llm are the same!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both rephrased questions, from langchain and by passing rephrase prompt to llm are the same!\n",
      "Both answers, from langchain and by passing final prompt to llm are the same!\n"
     ]
    }
   ],
   "source": [
    "prompts_dir = \"prompts/20250413_083110/How can we prevent forming them?\"\n",
    "test_prompts(prompts_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def extract_prompts(text):\n",
    "#     system_match = re.search(r'System:(.*?)Human:', text, re.DOTALL)\n",
    "#     human_match = re.search(r'Human:(.*?)$', text, re.DOTALL)\n",
    "    \n",
    "#     return {\n",
    "#         \"system\": system_match.group(1).strip() if system_match else None,\n",
    "#         \"human\": human_match.group(1).strip() if human_match else None\n",
    "#     }\n",
    "\n",
    "# def get_messages(prompt_dir):\n",
    "#     file_cnt = len(os.listdir(prompt_dir))\n",
    "#     if file_cnt == 3:\n",
    "#         lst = open(f\"{prompt_dir}/prompt_0.txt\").read().split(\"Human: \")[1:]\n",
    "\n",
    "#         rephrase_question = []\n",
    "\n",
    "#         for i, item in enumerate(lst):\n",
    "#             if i == 0:\n",
    "#                 rephrase_question.append({\"role\": \"user\", \"content\": item})\n",
    "#             elif i == len(lst)-1:\n",
    "#                 user_assistant = item.split(\"Assistant: \")\n",
    "#                 assistant_followup = user_assistant[1].split(\"\\n\")\n",
    "#                 assistant = \"\\n\".join(assistant_followup[:-2])\n",
    "#                 followup = \"\\n\".join(assistant_followup[-2:])\n",
    "#                 rephrase_question.append({\"role\": \"user\", \"content\": user_assistant[0]})\n",
    "#                 rephrase_question.append({\"role\": \"assistant\", \"content\": assistant})\n",
    "#                 rephrase_question.append({\"role\": \"user\", \"content\": followup})\n",
    "#             else:\n",
    "#                 user_assistant = item.split(\"Assistant: \")\n",
    "#                 rephrase_question.append({\"role\": \"user\", \"content\": user_assistant[0]})\n",
    "#                 rephrase_question.append({\"role\": \"assistant\", \"content\": user_assistant[1]})\n",
    "        \n",
    "#         answer_question = []\n",
    "\n",
    "#         prompt1 = open(f\"{prompt_dir}/prompt_1.txt\").read()\n",
    "#         result = extract_prompts(prompt1)\n",
    "#         answer_question.append({\"role\": \"system\", \"content\": result[\"system\"]})\n",
    "#         answer_question.append({\"role\": \"user\", \"content\": result[\"human\"]})\n",
    "\n",
    "#         return rephrase_question, answer_question\n",
    "\n",
    "#     else:\n",
    "#         answer_question = []\n",
    "\n",
    "#         prompt0 = open(f\"{prompt_dir}/prompt_0.txt\").read()\n",
    "#         result = extract_prompts(prompt0)\n",
    "#         answer_question.append({\"role\": \"system\", \"content\": result[\"system\"]})\n",
    "#         answer_question.append({\"role\": \"user\", \"content\": result[\"human\"]})\n",
    "\n",
    "#         return answer_question\n",
    "\n",
    "# prompt_dir = \"/home/vikas/Projects/Document-QA-System/tests/prompts/20250412_205246/How can we prevent forming them?\"\n",
    "# rephrase_question, answer_question = get_messages(prompt_dir)\n",
    "\n",
    "# rephrased_question = llm.invoke(rephrase_question).content\n",
    "\n",
    "# print(rephrased_question)\n",
    "\n",
    "# answer = llm.invoke(answer_question).content\n",
    "\n",
    "# print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
